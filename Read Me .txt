=》 Project Overview :
	We Have A Binary Classification Problem of defining whether the E-mail Is a spam or ham Email.

=》 DataSets :
	1) " dataset-1.csv " -> IS A labeled dataset consists of 5175  Record (or "spam and ham E-mails observations ").
	2) " dataset-2.csv " -> IS A labeled dataset consists of 5573  Record (or "spam and ham E-mails observations ").
	3) " dataset-3.csv " -> IS A labeled dataset consists of 30000 Record (or "spam and ham E-mails observations ").

=》 DataSets reprocessing :
	Our Three Datasets were four folders ('enron1 , enron2 , enron3 , enron4') each separated to spam file consists of spams.txt ('spam emails observations in txt files format') and the same for hams.txt ('ham emails observations  in txt files format') , so we made some processing steps on those folders for converting it to ('spam-ham.csv') file .
	=》 Firstly , we Imported the necessary libraris we have used :

**
	import os
	import pandas as pd 
	import codecs        # For Opening .txt files 
	import random
**
	
	=》 Then , defining A set for each class :

**
	ham = [[]]
	spam = [[]]
	emails = [[]]
**
	=》 Then , by two 'for' loops we have iterated to open those text files to read and append them respectevely at thier pre-defined lists :

**
	# Ham File
	for filename in os.listdir("enron4/enron4/ham/"):  
   	   file = open("enron4/enron4/ham/" + filename,"r+")
   	   ham.append([file.read(),"ham"])

	# Spam File
	for filename in os.listdir("enron4/enron4/spam/"):  
           with codecs.open("enron4/enron4/spam/" + filename, 'r', encoding='utf-8',errors='ignore') as fdata:
           spam.append([fdata.read(),"spam"])
**
	
	=》 Then , we have merged them togather at one defined list ('emails'):

**
	# Merge two Lists
	emails = ham + spam
**

	=》 Then , we shuffled this final list ('emails') :

**
	# Shuffl List
 	random.shuffle(emails)
**

	=》 Then , We converted the final shuffled list to A DataFrame By pandas package :

**
	# Create the pandas DataFrame 
	df = pd.DataFrame(emails, columns = ['text', 'label'])
**

	=》 Finally , we have to convert this dataframe to a ('csv') file and give A path to the converter function :
**
 	# Convert DF to CSV
	df.to_csv (r"E:\Heba's college\Level four\Semester Two\NLP\NLP-project\Data Sets\AnnDataset.csv", index = None, header=True)
**

*************************************************************************************************************************************
=》 Data Pre-Processing Before getting into Machine\Deep Learning Models :

	=》 for each Dataset , we loaded and dropped un-necessary columns and get some plots ('Bar chart , pie chart , histogram ') , adding additional column ('length') which identify the leanth of each e-mail observation , ploting the most thirty words common words in each (spam , ham), and encoding the two categorical classes to zeros and ones .
	=》 Then , we defined ('pre-processing') function wjich takes a text ('E-mail') and doing some processing upon like ('translate') function which takes a string and ('Panctuate') it that have to find all punctuation signs on this text and remove it ,then make sure the this words aren't from the language stopwords finally ('stemming') the rest words to get its origin without any grammatecal additions .

**
       def pre_process(text):
   	 text = text.translate(str.maketrans('', '', string.punctuation))
   	 text = [word for word in text.split() if word.lower() not in stopwords.words('english')]
    	 words = ""
    	 for i in text:
            stemmer = SnowballStemmer("english")
            words += (stemmer.stem(i)) + " "
    	return words
**

	=》 Then , getting to vectorize words what rests from previous function steps to be able to be learnt by machine\deep learning algorithms by ('tfidfvectorizer').

** 
	#Prprocessing for Data to find the best Acc and Vectorizing the data to be able to be learned
	textFeatures = data['text'].copy()
	textFeatures = textFeatures.apply(pre_process)
	vectorizer = TfidfVectorizer("english")
	features = vectorizer.fit_transform(textFeatures)
**
	
	=》 Then , we have to split our resulted Data after those all previous steps into training and test Data to learn the model and testing by the same dataset :

**
 	#Split Data after Preprocessing to Training set & Testing set with it's Labels
	features_train, features_test, labels_train, labels_test = train_test_split(features, data['class'], test_size=0.3,random_state=111)
**

	=》 Then , getting into learning several models to classify right our binary classification problem ( spam-or-ham ) all over datsets we had used .

*************************************************************************************************************************************

 
	